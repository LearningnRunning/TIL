{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "STATIC_PATH = '/home/elinha/NLP/moducon_data'\n",
    "north_america_file_name = 'review_data_japan_china_american.csv'\n",
    "\n",
    "north_america_path= os.path.join(STATIC_PATH, north_america_file_name)\n",
    "na_df = pd.read_csv(north_america_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 90.9M/90.9M [00:07<00:00, 11.5MB/s]\n",
      "model.onnx: 100%|██████████| 90.4M/90.4M [00:08<00:00, 10.5MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:08<00:00, 10.9MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 12.6kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 156kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.11MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 665kB/s]\n",
      "train_script.py: 100%|██████████| 13.2k/13.2k [00:00<00:00, 17.1MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 13.0MB/s]\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 665kB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "transformer_path = 'all-MiniLM-L6-v2'  # 5 times faster and still offers good quality\n",
    "embedder = SentenceTransformer(transformer_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = na_df['cleaned_sentence'].values.tolist()\n",
    "corpus_embeddings = embedder.encode(corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elinha/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 20\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(corpus_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
    "\n",
    "cluster_dict = {}\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    cluster_dict[f\"review_cluster_{i}\"] = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negavite_review_cluster_0 502\n",
      "negavite_review_cluster_1 228\n",
      "negavite_review_cluster_2 656\n",
      "negavite_review_cluster_3 652\n",
      "negavite_review_cluster_4 1197\n",
      "negavite_review_cluster_5 443\n",
      "negavite_review_cluster_6 823\n",
      "negavite_review_cluster_7 576\n",
      "negavite_review_cluster_8 366\n",
      "negavite_review_cluster_9 651\n",
      "negavite_review_cluster_10 611\n",
      "negavite_review_cluster_11 564\n",
      "negavite_review_cluster_12 479\n",
      "negavite_review_cluster_13 416\n",
      "negavite_review_cluster_14 354\n",
      "negavite_review_cluster_15 504\n",
      "negavite_review_cluster_16 1009\n",
      "negavite_review_cluster_17 919\n",
      "negavite_review_cluster_18 1144\n",
      "negavite_review_cluster_19 478\n"
     ]
    }
   ],
   "source": [
    "for k, v in cluster_dict.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_cluster_df = pd.DataFrame(\n",
    "    {\"cluster\": cluster_dict.keys(), \"sentences\": cluster_dict.values()}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = pd.DataFrame()\n",
    "\n",
    "for k,v in cluster_dict.items():\n",
    "    df = pd.DataFrame({\"review\": v})\n",
    "    df['cluster'] = k\n",
    "    cluster_df = pd.concat([cluster_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's so well written, the characters are all a...</td>\n",
       "      <td>negavite_review_cluster_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>desirous character is too good to be true.</td>\n",
       "      <td>negavite_review_cluster_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If Jules' husband was played by some actor wit...</td>\n",
       "      <td>negavite_review_cluster_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Both the stars acting compliments each other.</td>\n",
       "      <td>negavite_review_cluster_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The two stars are fantastic and well matched, ...</td>\n",
       "      <td>negavite_review_cluster_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>The rest of the cast was great too.</td>\n",
       "      <td>negavite_review_cluster_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>All the other main characters/actors were deli...</td>\n",
       "      <td>negavite_review_cluster_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>All around the actors who filled roles in this...</td>\n",
       "      <td>negavite_review_cluster_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>These shifts are executed very well through th...</td>\n",
       "      <td>negavite_review_cluster_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>From the brilliant acting, to the great direct...</td>\n",
       "      <td>negavite_review_cluster_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  \\\n",
       "0    It's so well written, the characters are all a...   \n",
       "1           desirous character is too good to be true.   \n",
       "2    If Jules' husband was played by some actor wit...   \n",
       "3        Both the stars acting compliments each other.   \n",
       "4    The two stars are fantastic and well matched, ...   \n",
       "..                                                 ...   \n",
       "473                The rest of the cast was great too.   \n",
       "474  All the other main characters/actors were deli...   \n",
       "475  All around the actors who filled roles in this...   \n",
       "476  These shifts are executed very well through th...   \n",
       "477  From the brilliant acting, to the great direct...   \n",
       "\n",
       "                        cluster  \n",
       "0     negavite_review_cluster_0  \n",
       "1     negavite_review_cluster_0  \n",
       "2     negavite_review_cluster_0  \n",
       "3     negavite_review_cluster_0  \n",
       "4     negavite_review_cluster_0  \n",
       "..                          ...  \n",
       "473  negavite_review_cluster_19  \n",
       "474  negavite_review_cluster_19  \n",
       "475  negavite_review_cluster_19  \n",
       "476  negavite_review_cluster_19  \n",
       "477  negavite_review_cluster_19  \n",
       "\n",
       "[12572 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = cluster_df.drop_duplicates(subset=['review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's so well written, the characters are all a...</td>\n",
       "      <td>negavite_review_cluster_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>desirous character is too good to be true.</td>\n",
       "      <td>negavite_review_cluster_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If Jules' husband was played by some actor wit...</td>\n",
       "      <td>negavite_review_cluster_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Both the stars acting compliments each other.</td>\n",
       "      <td>negavite_review_cluster_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The two stars are fantastic and well matched, ...</td>\n",
       "      <td>negavite_review_cluster_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>The rest of the cast was great too.</td>\n",
       "      <td>negavite_review_cluster_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>All the other main characters/actors were deli...</td>\n",
       "      <td>negavite_review_cluster_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>All around the actors who filled roles in this...</td>\n",
       "      <td>negavite_review_cluster_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>These shifts are executed very well through th...</td>\n",
       "      <td>negavite_review_cluster_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>From the brilliant acting, to the great direct...</td>\n",
       "      <td>negavite_review_cluster_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12566 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review  \\\n",
       "0    It's so well written, the characters are all a...   \n",
       "1           desirous character is too good to be true.   \n",
       "2    If Jules' husband was played by some actor wit...   \n",
       "3        Both the stars acting compliments each other.   \n",
       "4    The two stars are fantastic and well matched, ...   \n",
       "..                                                 ...   \n",
       "473                The rest of the cast was great too.   \n",
       "474  All the other main characters/actors were deli...   \n",
       "475  All around the actors who filled roles in this...   \n",
       "476  These shifts are executed very well through th...   \n",
       "477  From the brilliant acting, to the great direct...   \n",
       "\n",
       "                        cluster  \n",
       "0     negavite_review_cluster_0  \n",
       "1     negavite_review_cluster_0  \n",
       "2     negavite_review_cluster_0  \n",
       "3     negavite_review_cluster_0  \n",
       "4     negavite_review_cluster_0  \n",
       "..                          ...  \n",
       "473  negavite_review_cluster_19  \n",
       "474  negavite_review_cluster_19  \n",
       "475  negavite_review_cluster_19  \n",
       "476  negavite_review_cluster_19  \n",
       "477  negavite_review_cluster_19  \n",
       "\n",
       "[12566 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "              Write a summary of the following text delimited by triple backticks.\n",
    "              Return your response which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              SUMMARY:\n",
    "           \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_list = cluster_df[cluster_df['cluster'] == 'negavite_review_cluster_0']['review'].to_list()\n",
    "test_text = \" \".join(test_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61717"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.to_csv(os.path.join(STATIC_PATH, 'cluster_df.csv'), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from  langchain import LLMChain, HuggingFacePipeline, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=10000,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 1.63G/1.63G [02:28<00:00, 10.9MB/s]\n",
      "generation_config.json: 100%|██████████| 363/363 [00:00<00:00, 111kB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 2.49MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 856kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 1.48MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2298"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ARTICLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = summarizer(test_text[:2298], max_length=130, min_length=30, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Both the stars acting compliments each other. The only negative that I could grasp from this movie is the fact that all the younger people took a page from De tiro\\'s \"experience\" and started molding towards his way of life. They aren\\'t developed as characters, they\\'re just onlookers to their successes.'}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 1024  # LLM의 최대 길이 제한\n",
    "\n",
    "# 텍스트를 토큰 단위로 나누기\n",
    "tokens = tokenizer.encode(test_text)\n",
    "num_tokens = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_chain.run(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain_result= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(chunk_tokens)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)  \u001b[39m# 배치 차원 추가\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[39m=\u001b[39m summarizer(test_text, max_length\u001b[39m=\u001b[39;49m\u001b[39m130\u001b[39;49m, min_length\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m llm_chain_result\u001b[39m.\u001b[39mappend(outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/transformers/pipelines/text2text_generation.py:269\u001b[0m, in \u001b[0;36mSummarizationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39m          ids of the summary.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/transformers/pipelines/text2text_generation.py:167\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m         \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m)\n\u001b[1;32m    170\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(el, \u001b[39mstr\u001b[39m) \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m args[\u001b[39m0\u001b[39m])\n\u001b[1;32m    171\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result)\n\u001b[1;32m    172\u001b[0m     ):\n\u001b[1;32m    173\u001b[0m         \u001b[39mreturn\u001b[39;00m [res[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/transformers/pipelines/base.py:1196\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1189\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1190\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m         )\n\u001b[1;32m   1194\u001b[0m     )\n\u001b[1;32m   1195\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/transformers/pipelines/base.py:1203\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1202\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1203\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1204\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1205\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/transformers/pipelines/base.py:1102\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1101\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1102\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1103\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1104\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/transformers/pipelines/text2text_generation.py:191\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     in_b, input_length \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mshape(model_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_inputs(\n\u001b[1;32m    187\u001b[0m     input_length,\n\u001b[1;32m    188\u001b[0m     generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmin_length),\n\u001b[1;32m    189\u001b[0m     generate_kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmax_length),\n\u001b[1;32m    190\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m output_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    192\u001b[0m out_b \u001b[39m=\u001b[39m output_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/transformers/generation/utils.py:1413\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1406\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1407\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1408\u001b[0m         )\n\u001b[1;32m   1410\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1411\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1413\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   1414\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   1415\u001b[0m     )\n\u001b[1;32m   1417\u001b[0m \u001b[39m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/transformers/generation/utils.py:518\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    516\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    517\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 518\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[1;32m    520\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:1154\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens(input_ids) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_scale\n\u001b[0;32m-> 1154\u001b[0m embed_pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_positions(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1155\u001b[0m embed_pos \u001b[39m=\u001b[39m embed_pos\u001b[39m.\u001b[39mto(inputs_embeds\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   1157\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m embed_pos\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/transformers/models/bart/modeling_bart.py:135\u001b[0m, in \u001b[0;36mBartLearnedPositionalEmbedding.forward\u001b[0;34m(self, input_ids, past_key_values_length)\u001b[0m\n\u001b[1;32m    130\u001b[0m bsz, seq_len \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    131\u001b[0m positions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\n\u001b[1;32m    132\u001b[0m     past_key_values_length, past_key_values_length \u001b[39m+\u001b[39m seq_len, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    133\u001b[0m )\u001b[39m.\u001b[39mexpand(bsz, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(positions \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moffset)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.17/envs/nlp_env/lib/python3.9/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# 텍스트를 max_length 크기로 청크로 나누기\n",
    "for i in range(0, num_tokens, max_length):\n",
    "    chunk_tokens = tokens[i:i+max_length]\n",
    "    input_ids = torch.tensor(chunk_tokens).unsqueeze(0)  # 배치 차원 추가\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = summarizer(test_text, max_length=130, min_length=30, do_sample=False)\n",
    "    \n",
    "    # generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    llm_chain_result.append(outputs)\n",
    "\n",
    "# 결과 출력\n",
    "for idx, result in enumerate(llm_chain_result):\n",
    "    print(f\"Chunk {idx+1}:\")\n",
    "    print(result)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \"\"\"\\n            # Get the text\\n            text = self.text\\n            # Split text into paragraphs\\n            paragraphs = text.split(\"\\\\n\\\\n\")\\n            # Get the summary of the text\\n            summary = self.get_summary(paragraphs)\\n            # Return the summary\\n            return summary\\n\\n        # Define the function to get the summary of the text\\n        def get_summary(paragraphs):\\n            # Initialize the summary\\n            summary = \"\"\\n            # Iterate over the paragraphs\\n            for paragraph in paragraphs:\\n                # Check if the paragraph is not empty\\n                if paragraph:\\n                    # Split the paragraph into sentences\\n                    sentences = paragraph.split(\".\")\\n                    # Iterate over the sentences\\n                    for sentence in sentences:\\n                        # Check if the sentence is not empty\\n                        if sentence:\\n                            # Add the sentence to the summary\\n                            summary += sentence + \". \"\\n            # Return the summary\\n            return summary\\n\\n        # Call the function to get the summary of the text\\n        summary = get_summary(paragraphs)\\n\\n        # Return the summary\\n        return summary\\n    ```\\n\\n In this code, we first define a function called `get_summary` that takes a list of paragraphs as input and returns a summary of the text. The function iterates over the paragraphs and checks if each paragraph is not empty. If it is not empty, the function splits the paragraph into sentences and iterates over the sentences. It then checks if each sentence is not empty and adds it to the summary. Finally, the function returns the summary.\\n\\n We then define a function called `get_summary_from_text` that takes a string as input and returns a summary of the text. This function first splits the text into paragraphs and then calls the `get_summary` function to get the summary of the text. Finally, it returns the summary.\\n\\n In the main function, we create a string called `text` that contains the text we want to summarize. We then call the `get_summary_from_text` function to get the summary of the text. Finally, we print the summary to the console.\\n\\n The code is well documented with docstrings and comments to explain the logic and control flow of the code. The code is also structured in a way that makes it easy to read and understand.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "22953c844da62953e4170220ffa57db91a26ba8e65ca5dfcb0b44976e00b06c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
